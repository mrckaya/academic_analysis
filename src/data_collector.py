"""
Akademik YayÄ±n Verilerini Toplama ModÃ¼lÃ¼

Bu modÃ¼l, Semantic Scholar API kullanarak akademik yayÄ±n verilerini toplar.
Toplanan veriler:
- YayÄ±n baÅŸlÄ±klarÄ±
- Ã–zetler
- Yazar isimleri
- AtÄ±f sayÄ±larÄ±
- YayÄ±n URL'leri
"""

import time
import ast
import re
from typing import List, Dict, Optional
import pandas as pd
import requests


class ScholarDataCollector:
    """
    Akademik yayÄ±n verilerini toplama sÄ±nÄ±fÄ±
    
    Semantic Scholar API kullanarak akademik yayÄ±nlarÄ± arar ve toplar.
    Rate limiting ve retry mekanizmasÄ± ile gÃ¼venilir veri toplama saÄŸlar.
    """
    
    def __init__(
        self, 
        api_key: Optional[str] = None,
        delay: float = 3.0,
        timeout: float = 30.0
    ):
        """
        Args:
            api_key: Semantic Scholar API key (opsiyonel)
                     API key ile rate limit 5000 istek/5 dakika'ya Ã§Ä±kar
                     API key olmadan: 100 istek/5 dakika
            delay: Ä°stekler arasÄ± bekleme sÃ¼resi (saniye)
                   Rate limit'i aÅŸmamak iÃ§in minimum 3 saniye Ã¶nerilir
            timeout: Her API isteÄŸi iÃ§in timeout sÃ¼resi (saniye)
        """
        self.api_key = api_key
        self.delay = delay
        self.timeout = timeout
        self.collected_publications = []
        self.api_base_url = "https://api.semanticscholar.org/graph/v1"
        
        print("[OK] Semantic Scholar API modu aktif")
        if api_key:
            print("[OK] API key kullanÄ±lÄ±yor (yÃ¼ksek rate limit)")
    
    def _api_request(self, endpoint: str, params: Dict = None, max_retries: int = 3) -> Optional[Dict]:
        """
        Semantic Scholar API'ye HTTP isteÄŸi gÃ¶nderir
        
        Retry mekanizmasÄ± ile rate limiting ve timeout hatalarÄ±nÄ± yÃ¶netir.
        
        Args:
            endpoint: API endpoint (Ã¶rn: 'paper/search')
            params: Ä°stek parametreleri
            max_retries: Maksimum retry sayÄ±sÄ±
            
        Returns:
            API yanÄ±tÄ± (JSON dict) veya None (hata durumunda)
        """
        url = f"{self.api_base_url}/{endpoint}"
        headers = {
            'User-Agent': 'Academic Analysis Tool',
            'Accept': 'application/json'
        }
        
        if self.api_key:
            headers['x-api-key'] = self.api_key
        
        for attempt in range(max_retries):
            try:
                response = requests.get(url, params=params, headers=headers, timeout=self.timeout)
                
                # 429 hatasÄ± (Rate Limit) - bekleyip tekrar dene
                if response.status_code == 429:
                    retry_after = int(response.headers.get('Retry-After', 60))
                    wait_time = min(retry_after, 120)
                    
                    if attempt < max_retries - 1:
                        print(f"  [WAIT] Rate limit asildi! {wait_time} saniye bekleniyor... (Deneme {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"  [ERROR] Rate limit hatasi: {max_retries} deneme sonrasi basarisiz")
                        return None
                
                response.raise_for_status()
                return response.json()
                
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5
                    print(f"  [WAIT] Timeout! {wait_time} saniye bekleniyor... (Deneme {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"  [ERROR] Timeout hatasi: {max_retries} deneme sonrasi basarisiz")
                    return None
                    
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2
                    print(f"  [WAIT] Hata: {e}. {wait_time} saniye bekleniyor... (Deneme {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"  [ERROR] API istegi hatasi: {e}")
                    return None
        
        return None
    
    def _parse_api_paper(self, paper_data: Dict) -> Optional[Dict]:
        """
        API'den gelen yayÄ±n verisini standart formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
        
        Args:
            paper_data: API'den gelen ham yayÄ±n verisi (dict)
            
        Returns:
            Standart formatta yayÄ±n verisi (dict) veya None
        """
        # Yazar isimlerini Ã§Ä±kar
        authors = []
        if 'authors' in paper_data:
            authors = [author.get('name', '') for author in paper_data['authors'] if author.get('name')]
        
        # AtÄ±f sayÄ±sÄ±
        citation_count = paper_data.get('citationCount', 0) or 0
        
        # YayÄ±n URL'si (Ã¶ncelik: DOI, sonra direkt URL)
        url = ""
        if 'externalIds' in paper_data and 'DOI' in paper_data['externalIds']:
            url = f"https://doi.org/{paper_data['externalIds']['DOI']}"
        elif 'url' in paper_data:
            url = paper_data['url']
        
        return {
            'title': paper_data.get('title', ''),
            'abstract': paper_data.get('abstract', ''),
            'authors': authors,
            'citation_count': citation_count,
            'pub_url': url
        }
    
    def search_publications(
        self, 
        query: str, 
        max_results: int = 100
    ) -> List[Dict]:
        """
        Semantic Scholar API ile yayÄ±n arama
        
        Belirli bir sorgu ile akademik yayÄ±nlarÄ± arar ve toplar.
        Pagination ile birden fazla sayfa sonuÃ§larÄ±nÄ± toplar.
        
        Args:
            query: Arama sorgusu (Ã¶rn: "machine learning")
            max_results: Maksimum toplanacak yayÄ±n sayÄ±sÄ±
            
        Returns:
            YayÄ±n bilgilerini iÃ§eren liste
        """
        publications = []
        offset = 0
        limit = min(100, max_results)  # API limit: maksimum 100 sonuÃ§/istek
        
        print(f"  ğŸ” Semantic Scholar API'de aranÄ±yor: '{query}'")
        
        # Pagination ile tÃ¼m sonuÃ§larÄ± topla
        while len(publications) < max_results:
            # API istek parametreleri
            params = {
                'query': query,
                'limit': limit,
                'offset': offset,
                'fields': 'title,abstract,authors,citationCount,externalIds,url'
            }
            
            # API isteÄŸi gÃ¶nder
            response_data = self._api_request('paper/search', params)
            
            # Hata kontrolÃ¼
            if not response_data or 'data' not in response_data:
                break
            
            papers = response_data.get('data', [])
            if not papers:
                break
            
            # Her yayÄ±nÄ± parse et ve listeye ekle
            for paper in papers:
                if len(publications) >= max_results:
                    break
                
                pub_data = self._parse_api_paper(paper)
                if pub_data and pub_data['title']:
                    publications.append(pub_data)
                    print(f"  [OK] {len(publications)}/{max_results} yayÄ±n toplandÄ±: {pub_data['title'][:50]}...", end='\r')
            
            # Rate limiting iÃ§in bekle (minimum 3 saniye)
            time.sleep(max(self.delay, 3.0))
            
            # Sonraki sayfa iÃ§in offset artÄ±r
            offset += limit
            
            # EÄŸer daha az sonuÃ§ geldiyse, daha fazla sayfa yok demektir
            if len(papers) < limit:
                break
        
        print(f"\n  [OK] '{query}' sorgusu tamamlandÄ±: {len(publications)} yayÄ±n toplandÄ±")
        return publications
    
    def collect_multiple_queries(
        self, 
        queries: List[str], 
        max_results_per_query: int = 50
    ) -> pd.DataFrame:
        """
        Birden fazla sorgu ile veri toplar
        
        Her sorgu iÃ§in ayrÄ± ayrÄ± arama yapar ve sonuÃ§larÄ± birleÅŸtirir.
        Sorgular arasÄ± rate limiting iÃ§in bekleme sÃ¼resi ekler.
        
        Args:
            queries: Arama sorgularÄ± listesi
            max_results_per_query: Her sorgu iÃ§in maksimum sonuÃ§ sayÄ±sÄ±
            
        Returns:
            TÃ¼m sorgulardan toplanan yayÄ±nlarÄ±n DataFrame'i
        """
        all_publications = []
        
        print(f"\n[INFO] Toplam {len(queries)} sorgu iÅŸlenecek")
        print(f"[INFO] Her istek icin timeout: {self.timeout} saniye")
        print(f"[INFO] Her sorgu icin maksimum {max_results_per_query} sonuc\n")
        
        # Her sorgu iÃ§in arama yap
        for idx, query in enumerate(queries, 1):
            print(f"\n{'='*60}")
            print(f"Sorgu {idx}/{len(queries)}: '{query}'")
            print(f"{'='*60}")
            
            try:
                publications = self.search_publications(query, max_results_per_query)
                all_publications.extend(publications)
                print(f"[OK] Sorgu {idx} tamamlandÄ±: {len(publications)} yayÄ±n toplandÄ±")
            except Exception as e:
                print(f"[ERROR] Sorgu {idx} basarisiz: {e}")
                continue
            
            # Sorgular arasÄ± bekleme (API rate limit: 100 istek/5 dakika)
            if idx < len(queries):
                wait_time = 10  # Rate limit'i aÅŸmamak iÃ§in 10 saniye bekle
                print(f"[WAIT] Sonraki sorgu icin {wait_time} saniye bekleniyor...")
                time.sleep(wait_time)
        
        print(f"\n{'='*60}")
        print(f"[INFO] Toplam {len(all_publications)} yayin toplandi")
        print(f"{'='*60}\n")
        
        # DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
        if len(all_publications) > 0:
            df = pd.DataFrame(all_publications)
            self.collected_publications = all_publications
        else:
            print("[WARNING] Hic yayin toplanamadi! Bos DataFrame olusturuluyor...")
            df = pd.DataFrame()
        
        return df
    
    def save_to_csv(self, df: pd.DataFrame, filename: str = "publications.csv"):
        """
        Verileri CSV dosyasÄ±na kaydeder
        
        Args:
            df: Kaydedilecek DataFrame
            filename: Dosya yolu
        """
        df.to_csv(filename, index=False, encoding='utf-8')
        print(f"[OK] {len(df)} yayin {filename} dosyasina kaydedildi.")
    
    def load_from_csv(self, filename: str = "publications.csv") -> pd.DataFrame:
        """
        CSV dosyasÄ±ndan veri yÃ¼kler
        
        CSV'den okunan verileri parse eder:
        - authors kolonu string'den listeye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r
        - citation_count numeric'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r
        
        Args:
            filename: YÃ¼klenecek CSV dosyasÄ± yolu
            
        Returns:
            YÃ¼klenen verilerin DataFrame'i
        """
        import os
        
        # Dosya varlÄ±k kontrolÃ¼
        if not os.path.exists(filename):
            print(f"[WARNING] Dosya bulunamadi: {filename}")
            return pd.DataFrame()
        
        if os.path.getsize(filename) == 0:
            print(f"[WARNING] Dosya bos: {filename}")
            return pd.DataFrame()
        
        try:
            df = pd.read_csv(filename)
            
            if df.empty or len(df) == 0:
                print(f"[WARNING] CSV dosyasi bos: {filename}")
                self.collected_publications = []
                return pd.DataFrame()
            
            # authors kolonunu string'den listeye dÃ¶nÃ¼ÅŸtÃ¼r
            # CSV'ye kaydedilirken liste string'e dÃ¶nÃ¼ÅŸmÃ¼ÅŸ olabilir
            if 'authors' in df.columns:
                def parse_authors(author_str):
                    """Yazar string'ini listeye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r"""
                    if pd.isna(author_str) or author_str == '':
                        return []
                    if isinstance(author_str, list):
                        return author_str
                    if isinstance(author_str, str):
                        try:
                            # Python list formatÄ±nÄ± parse et: "['Author1', 'Author2']"
                            parsed = ast.literal_eval(author_str)
                            if isinstance(parsed, list):
                                return parsed
                        except (ValueError, SyntaxError):
                            # Manuel parse: kÃ¶ÅŸeli parantez iÃ§indeki isimleri Ã§Ä±kar
                            author_str = author_str.strip()
                            if author_str.startswith('[') and author_str.endswith(']'):
                                author_str = author_str[1:-1]
                                authors = re.findall(r"['\"]([^'\"]+)['\"]", author_str)
                                return [a.strip() for a in authors if a.strip()]
                            # VirgÃ¼lle ayrÄ±lmÄ±ÅŸ string ise
                            return [a.strip() for a in author_str.split(',') if a.strip()]
                    return []
                
                df['authors'] = df['authors'].apply(parse_authors)
            
            # citation_count'u numeric'e dÃ¶nÃ¼ÅŸtÃ¼r
            if 'citation_count' in df.columns:
                df['citation_count'] = pd.to_numeric(df['citation_count'], errors='coerce').fillna(0).astype(int)
            
            self.collected_publications = df.to_dict('records')
            print(f"[OK] {len(df)} yayin CSV'den yuklendi")
            return df
            
        except pd.errors.EmptyDataError:
            print(f"[WARNING] CSV dosyasi bos veya gecersiz: {filename}")
            return pd.DataFrame()
        except Exception as e:
            print(f"[ERROR] CSV yukleme hatasi: {e}")
            return pd.DataFrame()
